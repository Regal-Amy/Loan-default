# -*- coding: utf-8 -*-
"""loan_default (1).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/Regal-Amy/Loan-default/blob/main/customer-Loan-default-main/notebook/loan_default%20(1).ipynb
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv("/content/loan_default.csv")
df.head()

df.info()

df.describe()

"""**DATA CLEANING**

From the columns description above, it is apparent that some columns have high standard deviations and high max values beyond the 75%, hence the possible presence of outliers
"""

# Handling Outliers
df['CNT_CHILDREN'].value_counts()
outlier = df['CNT_CHILDREN'].quantile(0.99)
print(f"outlier: {outlier}")

df = df[df['CNT_CHILDREN'] <= outlier]

print("DataFrame after removing outlier:")
df['CNT_CHILDREN'].describe()

# Handling Outliers
median_value = df['YEARS_EMPLOYED'].median()
print(median_value)
df['YEARS_EMPLOYED'] = df['YEARS_EMPLOYED'].replace([-1000, -177.2222547, 150], median_value)

categorical_column = df.select_dtypes("object")
categorical_column.value_counts()

"""FLAG_OWN_REALTY has some inconsistencies in its column"""

# Handling inconsistencies in categorical columns
df.loc[:, 'FLAG_OWN_REALTY'] = df['FLAG_OWN_REALTY'].replace({'YES': 'Y', 'NO': 'N'})
df['FLAG_OWN_REALTY'].nunique()

# Handling null values
df.isnull().sum()

# Replace flag_own_car null values with the same proportion of N and Y as it was before
counts = df['FLAG_OWN_CAR'].value_counts()

total = df['FLAG_OWN_CAR'].notna().sum()  # Total non-null values

proportion_N = counts.iloc[0] / total
proportion_Y = counts.iloc[1] / total

num_nan = df['FLAG_OWN_CAR'].isna().sum()

# Generate random 'N' or 'Y' values based on  proportions
random_fill = np.random.choice(['N', 'Y'], size=num_nan, p=[proportion_N, proportion_Y])
df.loc[df['FLAG_OWN_CAR'].isna(), 'FLAG_OWN_CAR'] = random_fill

# Drop Target and flag_own_reality volumns
mask = df[['TARGET', 'FLAG_OWN_REALTY']].notna().all(axis=1)

# Replace only the rows where NaNs were dropped
df = df[mask]

mean_value = df['AMT_INCOME_TOTAL'].mean()
df['AMT_INCOME_TOTAL'].fillna(mean_value, inplace=True)

df.isnull().sum()

df['DAYS_BIRTH'] = (df['DAYS_BIRTH']//365).abs()
df['DAYS_BIRTH'].head()

df.drop_duplicates()
df.info()

df.describe()

categorical_column = df.select_dtypes("object")
categorical_column.value_counts()

"""**DATA  VISUALIZATION**"""

class_counts = df['TARGET'].value_counts()
total_count = len(df)
percentage = (class_counts / total_count) * 100

print(percentage)

def hist(data, title):
  figsize = (10,5)
  plt.hist(data, bins=10, edgecolor='black')
  plt.title(title)
  plt.xlabel('Value')
  plt.ylabel('Frequency')
  plt.show()

data = df.select_dtypes(include='number').columns

for col in data:
  hist(data=df[col], title= f'Distribution of {col}')

"""Most of the data are postively skewed"""

def boxPlot(column):
  sns.boxplot(x=df[column], orient='h')

  plt.title(column)
  plt.xlabel('AMT_ANNUITY')
  plt.show()

data = df.select_dtypes(include = 'number').columns
for col in data:
  boxPlot(col)

def stackplot(column, title):
  counts = df.groupby([column, 'TARGET']).size().unstack(fill_value=0)
  percentages = counts.div(counts.sum(axis=1), axis=0) * 100

  percentages.plot(kind='bar', stacked=True)
  plt.title(f'Percentage of {title} Loan Defaulters')
  plt.ylabel('Percentage')
  plt.xlabel(title)
  plt.show()

data = df.select_dtypes(include='object').columns
title = ['Contract Type', 'Gender', 'Car Owners', 'Realty Owners' ]
for col in data:
  stackplot(column = col, title = title[0])
  title.pop(0)

df.describe()

"""The categorical dataset indicates that;

*   The dataset contains more Male defaulters
*   Although not a significant difference, applicants who don't own a car default more than applicants who do
*   Also not a significant difference but applicants who don't own real estate default more than applicants who do
*   Applicants who get Cash loans are more liable to default than applicants who get Revolving loans






"""

sns.scatterplot(x='AMT_INCOME_TOTAL', y='AMT_CREDIT', data=df)
plt.title('AMT_INCOME_TOTAL vs AMT_CREDIT')
plt.ylabel('AMT_INCOME_TOTAL')
plt.xlabel('AMT_CREDIT')
plt.show()

"""Regardless of income, majority applicants got loan below 1,500,000"""

sns.barplot(x='CODE_GENDER', y='AMT_INCOME_TOTAL', data=df)
plt.title('GENDER vs AMT_INCOME_TOTAL')
plt.ylabel('CODE_GENDER')
plt.xlabel('AMT_INCOME_TOTAL')
plt.show()

"""Males get more loan than females"""

correlation = df.select_dtypes(include='number').corr()
sns.heatmap(correlation)

# Check correlation between features and the target
correlation = df.select_dtypes(include='number').corr()
print(correlation['TARGET'].sort_values(ascending=False))

df.drop(columns='AMT_ANNUITY', inplace=True)

"""**MODEL**"""

from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from imblearn.under_sampling import RandomUnderSampler
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import cross_val_score

y = df['TARGET']
X = df.drop(columns = 'TARGET')
print(f'X.shape: {X.shape}')
print(f'y.shape: {y.shape}')

Category = df.select_dtypes(include='object').columns

encoder = OneHotEncoder(sparse_output=False)
encoded_data = encoder.fit_transform(df[Category])

encoded_X = pd.DataFrame(encoded_data, columns=encoder.get_feature_names_out(Category), index=df.index)

X = pd.concat([df, encoded_X], axis=1).drop(Category, axis=1)

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)
X_scaled.shape

X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

print(f'X_train: {X_train.shape}')
print(f'X_test: {X_test.shape}')
print(f'y_train: {y_train.shape}')
print(f'y_test: {y_test.shape}')

rus = RandomUnderSampler(random_state=42)
X_train_resampled, y_train_resampled = rus.fit_resample(X_train, y_train)

clf = LogisticRegression()

clf.fit(X_train_resampled, y_train_resampled)

y_pred = clf.predict(X_test)

scores = cross_val_score(clf, X_train_resampled, y_train_resampled, cv=5)


print(f'Cross-Validation Scores: {scores}')

print(f'Average Cross-Validation Score: {scores.mean():.2f}')

accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred)
recall = recall_score(y_test, y_pred)
f1 = f1_score(y_test, y_pred)

print(f"Accuracy: {accuracy:.2f}")
print(f"Precision: {precision:.2f}")
print(f"Recall: {recall:.2f}")
print(f"F1 Score: {f1:.2f}")

cm = confusion_matrix(y_test, y_pred)

plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title('Confusion Matrix')
plt.xlabel('Predicted Labels')
plt.ylabel('True Labels')
plt.show()